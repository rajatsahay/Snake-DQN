{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Snake.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaql8AoNzE3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ad2074b-c167-4bb6-dedf-40be2d8d321f"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Dropout\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from operator import add"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvhhSM9OOJVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "5ee689b4-5279-4914-bd5d-3cec355b5c93"
      },
      "source": [
        "class DQNAgent(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    #Initializations\n",
        "    self.reward=0\n",
        "    self.gamma=0.9\n",
        "    self.dataframe=pd.DataFrame()\n",
        "    self.short_memory=np.array([])\n",
        "    self.agent_target=1\n",
        "    self.agent_predict=0\n",
        "    self.learning_rate=0.0005\n",
        "    self.model=self.network(\"weights.hdf5\") #found on github repo:\n",
        "    self.epsilon=0\n",
        "    self.actual=[]\n",
        "    self.memory=[]\n",
        "    \n",
        "  def get_state(self,game,player,food):\n",
        "    #states for all positions of each item of the game\n",
        "    \n",
        "     state = [\n",
        "         (player.x_change == 20 and player.y_change == 0 and ((list(map(add, player.position[-1], [20, 0])) in player.position) or\n",
        "         player.position[-1][0] + 20 >= (game.game_width - 20))) or (player.x_change == -20 and player.y_change == 0 and ((list(map(add, player.position[-1], [-20, 0])) in player.position) or\n",
        "         player.position[-1][0] - 20 < 20)) or (player.x_change == 0 and player.y_change == -20 and ((list(map(add, player.position[-1], [0, -20])) in player.position) or\n",
        "         player.position[-1][-1] - 20 < 20)) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add, player.position[-1], [0, 20])) in player.position) or\n",
        "         player.position[-1][-1] + 20 >= (game.game_height-20))),  # danger straight\n",
        "\n",
        "          (player.x_change == 0 and player.y_change == -20 and ((list(map(add,player.position[-1],[20, 0])) in player.position) or\n",
        "          player.position[ -1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],\n",
        "          [-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == -20 and player.y_change == 0 and ((list(map(\n",
        "          add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
        "          (list(map(add,player.position[-1],[0,20])) in player.position) or player.position[-1][\n",
        "          -1] + 20 >= (game.game_height-20))),  # danger right\n",
        "\n",
        "          (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],[20,0])) in player.position) or\n",
        "          player.position[-1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == -20 and ((list(map(\n",
        "          add, player.position[-1],[-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
        "          (list(map(add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (\n",
        "          player.x_change == -20 and player.y_change == 0 and ((list(map(add,player.position[-1],[0,20])) in player.position) or\n",
        "          player.position[-1][-1] + 20 >= (game.game_height-20))), #danger left\n",
        "         \n",
        "         \n",
        "          player.x_change==-20 #left\n",
        "          player.x_change==20 #right\n",
        "          player.y_change==-20 #up\n",
        "          player.y_change==20 #down\n",
        "          food.x_food < player.x #food left\n",
        "          food.x_food > player.x #food_right\n",
        "          food.y_food < player.y #food up\n",
        "          food.y_food > player.y #food down\n",
        "     ]\n",
        "      \n",
        "    for i in range(len(state)):\n",
        "      if state[i]:\n",
        "        state[i]=1\n",
        "      else:\n",
        "        state[i]=0\n",
        "      \n",
        "    return np.asarray(state)\n",
        "  \n",
        "  def set_reward(self,player,crash):\n",
        "    self.reward=0\n",
        "    if crash:\n",
        "      #deduct\n",
        "      self.reward=-10\n",
        "      return self.reward\n",
        "    if player.eaten:\n",
        "      #award\n",
        "      self.reward=10\n",
        "    return self.reward\n",
        "  \n",
        "  def network(self,weights=None):#weights from github were included in init()\n",
        "    #DQN\n",
        "    model=Sequential()\n",
        "    model.add(Dense(output_dim=120,activation='relu',input_dim=11))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(output_dim=120,activation='relu'))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(output_dim=120,activation='relu'))\n",
        "    model.add(Dropout(0.15))\n",
        "    model.add(Dense(output_dim=3,activation='softmax'))\n",
        "    model.compile(loss='mse',optimizer=Adam(self.learning_rate)) #Mean Squared Error\n",
        "    \n",
        "    if weights:\n",
        "      model.load_weights(weights)\n",
        "    return model\n",
        "  \n",
        "  def remember(self,state,action,rewardnext_state,done):\n",
        "    self.memory.append((state,action,reward,next_state,done))\n",
        "    \n",
        "  def replay_new(self,memory):\n",
        "    if len(memory)>1000:\n",
        "      minibatch=random.sample(memory,1000)\n",
        "    else:\n",
        "      minibatch=memory\n",
        "    \n",
        "    for state,action,reward,next_state, done in minibatch:\n",
        "      target=reward\n",
        "      if not done:\n",
        "        target=reward+self.gamma*np.amax(self.model.predict(np.array([next_state]))[0])\n",
        "        \n",
        "      target_f=self.model.predict(np.array([state]))\n",
        "      target_f[0][np.argmax(action)]=target\n",
        "      self.model.fit(np.array([state]),target_f,epochs=1,verbose=0)\n",
        "      \n",
        "  def train_short_memory(self,state,action,reward,next_state,done):\n",
        "    target=reward\n",
        "    if not done:\n",
        "      target=reward + self.gamma*np.amax(self.model.predict(next_shape.reshape((1,11)))[0])\n",
        "    \n",
        "    target_f=self.model.predict(state.reshape((1,11)))\n",
        "    target_f[0][np.argmax(action)]=target\n",
        "    self.model.fit(state.reshape((1,11)),target_f,epochs=1,verbose=0)\n",
        "    \n",
        "  \n",
        "         \n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f32c56db582f>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    player.x_change==20 #right\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlqRcAaITwl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}